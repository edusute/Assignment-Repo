{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff755ae-97af-412d-bac5-82be83729bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 23.2.1 from /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages/pip (python 3.11)\n"
     ]
    }
   ],
   "source": [
    "!pip --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125e5620-9551-4d0a-be83-668b9958950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Obtaining dependency information for kagglehub from https://files.pythonhosted.org/packages/a4/8e/4077b08b95a1f8302c694a8b399bd413815fbe89045c41e6e08cd7d9439a/kagglehub-0.3.13-py3-none-any.whl.metadata\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from kagglehub) (23.1)\n",
      "Requirement already satisfied: pyyaml in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from kagglehub) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from requests->kagglehub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from requests->kagglehub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryanmcmaster/anaconda3/lib/python3.11/site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd98af-25e6-47a2-b614-0a3482e724a7",
   "metadata": {},
   "source": [
    "**Note:** Install `kagglehub` to enable downloading Kaggle datasets directly from Python.  You only need to run this installation once per environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422fde50-27f7-4719-ba4b-c01bb4c7614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sakshisatre/titanic-dataset?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59.2k/59.2k [00:00<00:00, 918kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/ryanmcmaster/.cache/kagglehub/datasets/sakshisatre/titanic-dataset/versions/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sakshisatre/titanic-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64d4f7-9606-44f7-bff8-479615d3134f",
   "metadata": {},
   "source": [
    "**Note:** `kagglehub` lets you download Kaggle datasets directly in Python. It stores the files in your `~/.cache/kagglehub` folder so your project stays clean and the data doesn't need to be downloaded again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475faca-3997-4b54-98c6-96c01dc6bd53",
   "metadata": {},
   "source": [
    "# Titanic ML Data Dictionary (what we'll end up using)\n",
    "\n",
    "| Column     | Meaning                                 | Type         |\n",
    "| ---------- | --------------------------------------- | ------------ |\n",
    "| `survived` | 1 = passenger lived, 0 = passenger died | int (binary) |\n",
    "\n",
    "| Feature    | Description                                              | Type     | Notes                                  |\n",
    "| ---------- | -------------------------------------------------------- | -------- | -------------------------------------- |\n",
    "| `pclass`   | Passenger class (1 = first class, 2 = second, 3 = third) | int      | Strong survival predictor              |\n",
    "| `sex`      | Passenger sex (“male”, “female”)                         | category | Must be converted with get_dummies     |\n",
    "| `age`      | Passenger age in years                                   | float    | Some missing values → fill with median |\n",
    "| `sibsp`    | Number of siblings/spouses aboard                        | int      | Helps show family travel groups        |\n",
    "| `parch`    | Number of parents/children aboard                        | int      | Also family grouping                   |\n",
    "| `fare`     | Ticket price passenger paid                              | float    | Higher fare = wealthier passengers     |\n",
    "| `embarked` | Port of boarding (“C”, “Q”, “S”)                         | category | Also needs get_dummies                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aec9f0-0433-4c7c-8285-96df08c256ad",
   "metadata": {},
   "source": [
    "# 0.  INSTALL LIBRARIES  (If needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a93c91-052d-4059-9256-f8886014d665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDecision Tree Example in Python\\n\\nGoal:\\n- ....\\n\\nTools we use:\\n- pandas: to load and look at the data\\n- scikit-learn: to build and test the decision tree model\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decision Tree Example in Python\n",
    "\n",
    "Goal:\n",
    "- ....\n",
    "\n",
    "Tools we use:\n",
    "- pandas: to load and look at the data\n",
    "- scikit-learn: to build and test the decision tree model\n",
    "\"\"\"\n",
    "\n",
    "# You only need to run this ONCE in your environment.\n",
    "# In Jupyter, you can remove the # below and run this cell if scikit-learn isn't installed.\n",
    "# !pip install scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd30433-55d5-4640-aaf1-902db67b6c12",
   "metadata": {},
   "source": [
    "# 1. IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4321da8-fa77-4598-9999-3b7c312e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for working with tables (dataframes)\n",
    "from sklearn.model_selection import train_test_split  # to split data into train and test sets\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree  # our decision tree model + plotting function\n",
    "from sklearn.metrics import accuracy_score, classification_report  # to evaluate the model\n",
    "import matplotlib.pyplot as plt  # to draw the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45266290-6422-468a-afd8-6bc51cc1602c",
   "metadata": {},
   "source": [
    "# 2. LOAD AND INSPECT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b24cb1-4e56-4430-b4d8-7614fe40debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   pclass  survived                                             name     sex  \\\n",
      "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
      "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
      "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
      "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
      "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
      "\n",
      "     age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
      "0  29.00      0      0   24160  211.3375       B5        S    2    NaN   \n",
      "1   0.92      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
      "2   2.00      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "3  30.00      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
      "4  25.00      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "\n",
      "                         home.dest  \n",
      "0                     St Louis, MO  \n",
      "1  Montreal, PQ / Chesterville, ON  \n",
      "2  Montreal, PQ / Chesterville, ON  \n",
      "3  Montreal, PQ / Chesterville, ON  \n",
      "4  Montreal, PQ / Chesterville, ON  \n",
      "\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   object \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       486 non-null    object \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 143.3+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Basic statistics for numeric columns:\n",
      "            pclass     survived          age        sibsp        parch  \\\n",
      "count  1309.000000  1309.000000  1046.000000  1309.000000  1309.000000   \n",
      "mean      2.294882     0.381971    29.881138     0.498854     0.385027   \n",
      "std       0.837836     0.486055    14.413493     1.041658     0.865560   \n",
      "min       1.000000     0.000000     0.170000     0.000000     0.000000   \n",
      "25%       2.000000     0.000000    21.000000     0.000000     0.000000   \n",
      "50%       3.000000     0.000000    28.000000     0.000000     0.000000   \n",
      "75%       3.000000     1.000000    39.000000     1.000000     0.000000   \n",
      "max       3.000000     1.000000    80.000000     8.000000     9.000000   \n",
      "\n",
      "              fare        body  \n",
      "count  1308.000000  121.000000  \n",
      "mean     33.295479  160.809917  \n",
      "std      51.758668   97.696922  \n",
      "min       0.000000    1.000000  \n",
      "25%       7.895800   72.000000  \n",
      "50%      14.454200  155.000000  \n",
      "75%      31.275000  256.000000  \n",
      "max     512.329200  328.000000  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure 'Titanic Dataset.csv' is in the SAME folder as your/this notebook.\n",
    "df = pd.read_csv(\"Titanic Dataset.csv\")  # Read the CSV file we created earlier.\n",
    "\n",
    "# Show the first few rows so we can see what the data looks like.\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check basic info (number of rows, columns, and data types)\n",
    "print(\"Dataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Optional: See simple statistics (min, max, average, etc.) for numeric columns\n",
    "print(\"Basic statistics for numeric columns:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbd64a-a321-447d-b202-701663deec7d",
   "metadata": {},
   "source": [
    "# 3. SELECT USEFUL COLUMNS/FEATURES/VARIABLES (ALL THE SAME THING) FOR THE MACHINE LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb961f-ca3d-4dcc-ad01-e007ef57241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalVariables = [\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\", \"survived\"]  #creating a list for the variables/features that we want to keep\n",
    "\n",
    "df_model = df[finalVariables].copy()   # Make a clean working copy of the original dataset but assigning ONLY the features that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cd234-b623-4cbf-af75-b7e780fa0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at df_model\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fe913-3e73-494d-9a8e-1fb72095a66a",
   "metadata": {},
   "source": [
    "**NOTE**: If we want to see all rows or all columns in a dataframe in Pandas, we can change the settings.  You might want to consider first how many rows you have... \n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_rows', None)  # set rows to max in the view, or choose a number like \"10\" in place of \"None\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # same with columns\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28329e-6d49-4a54-8281-68068776f124",
   "metadata": {},
   "source": [
    "# 4: HANDLE MISSING VALUES IN THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b7cfb-c4d4-4757-931a-7333b797ad0f",
   "metadata": {},
   "source": [
    "## Checking for Missing Values with `.isnull()`\n",
    "\n",
    "Before training a machine learning model, it's important to understand whether our dataset has any **missing values**. Missing data can confuse the model, reduce accuracy, or even cause errors during training.\n",
    "\n",
    "### What `.isnull()` Does\n",
    "`df.isnull()` creates a table of **True/False values**:\n",
    "\n",
    "- **True** means the value is missing (`NaN`)\n",
    "- **False** means the value is present\n",
    "\n",
    "From there, we can count or calculate the percentage of missing values in each column.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "df_model.isnull().sum()        # total missing values per column\n",
    "df_model.isnull().mean() * 100 # percentage of missing values\n",
    "\n",
    "```\n",
    "\n",
    "## Handling Missing Values (Simple Overview)\n",
    "\n",
    "When real datasets have missing information, data scientists use different strategies to clean the data. Some common options include:\n",
    "\n",
    "* **Dropping rows** that contain missing values\n",
    "* **Dropping columns** that are mostly empty\n",
    "* **Filling in (imputing)** missing values using the **mean**, **median**, **mode**, or more advanced methods (like **KNN** or **regression-based imputation**)\n",
    "\n",
    "### Why We Use the Median in This Project\n",
    "\n",
    "For this assignment, we will fill missing numeric values (like `age` and `fare`) using the **median**.\n",
    "The median is a strong beginner-friendly choice because:\n",
    "\n",
    "* It’s **not influenced by extreme values**\n",
    "* It represents a **typical, central value** in the data\n",
    "* It keeps the dataset size the **same** (we don’t lose passengers)\n",
    "* It makes our Decision Tree model **simpler, more stable, and easy to interpret**\n",
    "\n",
    "### More Advanced Options (for later)\n",
    "\n",
    "There *are* smarter ways to fill in missing values. For example:\n",
    "\n",
    "* **Group-based imputation** (e.g., fill missing ages *within each passenger class*)\n",
    "* **Age buckets** (child, teen, adult, etc.)\n",
    "* **KNN imputation**, which finds **similar passengers** and uses their values\n",
    "* **Regression imputation**, which predicts missing values using other features\n",
    "\n",
    "These methods can produce more realistic results, but they require more advanced preprocessing.\n",
    "\n",
    "### For This Assignment\n",
    "\n",
    "We use **median imputation** because it is\n",
    "➡️ simple\n",
    "➡️ reliable\n",
    "➡️ and perfect for learning the *core machine learning workflow* without extra complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50edf37f-aaf1-43f7-a324-a8c375be2ade",
   "metadata": {},
   "source": [
    "## First let's look at how many missing values we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509236cc-3949-4f4d-aee7-de6ae111b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary table of missing values.\n",
    "missing_summary = pd.DataFrame({\n",
    "    # Total missing values in each column\n",
    "    \"missing_count\": df_model.isnull().sum(),\n",
    "    \n",
    "    # Percentage of missing values in each column\n",
    "    \"missing_percent\": df_model.isnull().mean() * 100\n",
    "})\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb3e3-5d2d-4f96-910a-13ca2ec4ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numeric values with the median\n",
    "df_model[\"age\"] = df_model[\"age\"].fillna(df_model[\"age\"].median())\n",
    "df_model[\"fare\"] = df_model[\"fare\"].fillna(df_model[\"fare\"].median())\n",
    "\n",
    "# Fill missing categorical values with the mode (most common value)\n",
    "df_model[\"embarked\"] = df_model[\"embarked\"].fillna(df_model[\"embarked\"].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a398c-def4-473d-8ae2-2433b49cc444",
   "metadata": {},
   "source": [
    "## Let's look at the model again: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22a711-3d78-4b24-bd0a-5b8f2fc1b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72cc3d-76b2-4ccb-838f-ec26fdb70f6e",
   "metadata": {},
   "source": [
    "## Let's check the missing values now that we've dealt with them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692650a0-16d2-4ce2-a557-fd60188801f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary table of missing values.\n",
    "missing_summary = pd.DataFrame({\n",
    "    # Total missing values in each column\n",
    "    \"missing_count\": df_model.isnull().sum(),\n",
    "    \n",
    "    # Percentage of missing values in each column\n",
    "    \"missing_percent\": df_model.isnull().mean() * 100\n",
    "})\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b28bb8-698d-4312-b749-18f88d9dfde8",
   "metadata": {},
   "source": [
    "# 5: CONVERT CATEGORICAL VARIABLES TO DUMMY VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a0cec-5cb4-46c4-b467-a86d842a5f74",
   "metadata": {},
   "source": [
    "## Converting Categorical Variables to Dummy Variables (Titanic)\n",
    "\n",
    "Before training a model, all features must be numeric. Scikit-learn cannot use text categories, so we prepare the data as follows:\n",
    "\n",
    "- **Keep:** `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `embarked`\n",
    "- **Drop:** `name`, `ticket`, `cabin`, `boat`, `body`, `home.dest`\n",
    "- **Convert text to numbers:** use `pd.get_dummies()` on `sex` and `embarked`\n",
    "- **Handle missing values:** fill missing `age` and `fare` with the median\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8877d-9b18-45aa-b7b8-dd0587a5562f",
   "metadata": {},
   "source": [
    "## What `get_dummies()` Does\n",
    "\n",
    "As we just read - for the most part, machine learning models want to work with numbers, not text (unless we're doing natural language processing).  \n",
    "`pd.get_dummies()` converts categorical (text) columns into **dummy variables**, which are numeric 0/1 columns.\n",
    "\n",
    "For example:\n",
    "\n",
    "- `sex` → `sex_male` (0 = female, 1 = male)\n",
    "- `embarked` → `embarked_Q`, `embarked_S`  # creating new variables (from one variable) and assigning them a value of 1 or 0\n",
    "\n",
    "Each dummy column represents one possible category.\n",
    "\n",
    "### Do I need to list which columns to convert?\n",
    "Not always.\n",
    "\n",
    "- If your DataFrame already contains **only the features you want**, you can simply run:\n",
    "\n",
    "```python\n",
    "  df_model = pd.get_dummies(df_model, drop_first=True)\n",
    "```\n",
    "\n",
    "**NOTE about `drop_first=True`:**  \n",
    "When a categorical feature is converted into multiple dummy columns, one of the columns is always redundant because its value can be inferred from the others. This redundancy can confuse some models. Using `drop_first=True` removes one dummy column per group, reducing duplication while still preserving all category information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee01bdc-9a2c-4c0f-b9ea-45770a226fe5",
   "metadata": {},
   "source": [
    "## Finally creating the dummies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60ea39-e97f-4759-8454-aa433806105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.get_dummies(df_model, drop_first=True)  # convert text categories into numeric dummy (0/1) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321efe05-4f4a-4f3e-91c9-5f507560d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()  \n",
    "\n",
    "# refresh from earlier... `df_model.head()` shows the first 5 rows as a default parameter, so we can preview the cleaned dataset\n",
    "# `df_model.head(10)` shows ten rows, and so on (fill in the parameter with the number of rows you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bd5bf-6380-4d76-9139-7fa76f14bb91",
   "metadata": {},
   "source": [
    "# 6: CHOOSE FEATURES (X) AND TARGET (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb9308-6023-4464-b843-ab165dc22476",
   "metadata": {},
   "source": [
    "Now that our data is cleaned and encoded, we need to tell the model:\n",
    "\n",
    "- **What inputs to use** to make a prediction (features `X`)\n",
    "- **What we want to predict** (target `y`)\n",
    "\n",
    "For the Titanic model:\n",
    "\n",
    "- **Target (`y`)**: `survived` (1 = lived, 0 = died)\n",
    "- **Features (`X`)**:  \n",
    "  - `pclass` — passenger class (1, 2, 3)  \n",
    "  - `age`  \n",
    "  - `sibsp` — siblings/spouses aboard  \n",
    "  - `parch` — parents/children aboard  \n",
    "  - `fare`  \n",
    "  - `sex_male` — dummy variable (1 = male, 0 = female)  \n",
    "  - `embarked_Q`, `embarked_S` — dummy variables for port of embarkation  \n",
    "    (if both are 0, that means Embarked = C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af72dbd-f1e7-4a40-9a70-aa3bfa99b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CHOOSE FEATURES AND TARGET (LABEL)\n",
    "\n",
    "# List of feature column names we want the model to use as inputs\n",
    "feature_names = [\n",
    "    \"pclass\",\n",
    "    \"age\",\n",
    "    \"sibsp\",\n",
    "    \"parch\",\n",
    "    \"fare\",\n",
    "    \"sex_male\",\n",
    "    \"embarked_Q\",\n",
    "    \"embarked_S\",\n",
    "]\n",
    "\n",
    "# X = all the input features (independent variables)\n",
    "X = df_model[feature_names]\n",
    "\n",
    "# y = the target we want to predict (dependent variable)\n",
    "# survived: 1 = lived, 0 = died\n",
    "y = df_model[\"survived\"]\n",
    "\n",
    "# Show a small sample so we can visually confirm X and y\n",
    "print(\"Features (X) sample:\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget (y) sample:\")\n",
    "print(y.head())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e612148-2b35-48ac-8d88-c4b26de16dab",
   "metadata": {},
   "source": [
    "## 7: TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88731c0-4b83-4876-83f3-1622e4d3beec",
   "metadata": {},
   "source": [
    "We don’t want to test the model on the same data it learned from.\n",
    "\n",
    "So we:\n",
    "\n",
    "- Use **80%** of the rows for **training** (teaching the model)\n",
    "- Use **20%** of the rows for **testing** (seeing how well it generalizes)\n",
    "\n",
    "`random_state=42` just makes the split reproducible (same split every run).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647a8f5-418c-4035-ba48-4cbaa08b7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SPLIT INTO TRAIN AND TEST SETS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set (returned as four arrays: X_train, X_test, y_train, y_test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                # features\n",
    "    y,                # target\n",
    "    test_size=0.2,    # 20% for testing, 80% for training\n",
    "    random_state=42   # fixed seed so results are reproducible\n",
    ")\n",
    "\n",
    "# Print how many rows ended up in each set\n",
    "print(\"Number of training rows:\", len(X_train))\n",
    "print(\"Number of test rows:\", len(X_test))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d51c6e-4826-439b-82fa-f68e360f4148",
   "metadata": {},
   "source": [
    "## 8: CREATE AND TRAIN THE DECISION TREE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e0b30-4297-4120-a821-65cd8ba67483",
   "metadata": {},
   "source": [
    "Now we:\n",
    "\n",
    "- Create a **DecisionTreeClassifier**\n",
    "- Limit the depth (e.g., `max_depth=3`) so it doesn’t memorise everything\n",
    "- **Fit** (train) the model using the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc02d3-32e5-4574-b88e-735069afdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. CREATE AND TRAIN THE DECISION TREE MODEL\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import the DecisionTreeClassifier class from scikit-learn.\n",
    "# scikit-learn (sklearn) is a machine learning library in Python.\n",
    "# The \"tree\" module contains tools for building decision trees.\n",
    "# DecisionTreeClassifier is the specific model we use to classify data\n",
    "# (i.e., predict categories like survived vs. not survived).\n",
    "\n",
    "\n",
    "# Create the decision tree model\n",
    "# max_depth controls how deep the tree can go (helps prevent overfitting)\n",
    "model = DecisionTreeClassifier(\n",
    "    random_state=42,  # reproducible tree\n",
    "    max_depth=3       # limit depth to keep the tree simple and interpretable\n",
    ")\n",
    "\n",
    "# Train (\"fit\") the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision tree model has been trained!\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12161e-9aff-40a9-962e-eff2b3773eba",
   "metadata": {},
   "source": [
    "## Step 9: Make Predictions and Compare to the True Labels\n",
    "\n",
    "We now:\n",
    "\n",
    "- Use the trained model to predict survival on the **test set**\n",
    "- Print the model’s predictions and the real outcomes side by side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17e3aa-d7ca-4b8e-9b0d-c7888af41f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. MAKE PREDICTIONS ON THE TEST DATA\n",
    "\n",
    "# Use the trained model to predict survival for the passengers in X_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions for test set (model's guesses):\")\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"True labels for test set (actual outcomes):\")\n",
    "print(y_test.values)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3beb07f-322a-4b7d-86b1-3101341ae38f",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6840f12-15b1-4d2f-aa19-aeb6a941d05e",
   "metadata": {},
   "source": [
    "### Understanding the Classification Report... The Output We'll Create (Simple Overview)\n",
    "\n",
    "The `classification_report` (a function from scikit-learn) summarizes how well a model performs on each class.\n",
    "\n",
    "### Key Terms (General Definitions)\n",
    "\n",
    "- **Precision**  \n",
    "  Of the predictions the model said were a certain class, how many were *actually* that class?  \n",
    "  *“When the model predicts **survived**, how often is it right?”*\n",
    "\n",
    "- **Recall**  \n",
    "  Of all the true cases of a class, how many did the model correctly identify?  \n",
    "  *“Out of all the people who **survived**, how many did the model detect?”*\n",
    "\n",
    "- **F1-score**  \n",
    "  The balanced average of precision and recall.  \n",
    "  Useful when classes are imbalanced.\n",
    "\n",
    "- **Accuracy**  \n",
    "  The percentage of all predictions that were correct.\n",
    "\n",
    "- **Macro Average**  \n",
    "  The average precision/recall/F1 across all classes, treating each class equally (even if one class is smaller).\n",
    "\n",
    "- **Weighted Average**  \n",
    "  Like macro average, but gives more weight to classes with more examples.  \n",
    "  More representative when the dataset is imbalanced.\n",
    "\n",
    "### What \"Support\" Means in the Table\n",
    "\n",
    "- **Support** = the number of true examples of each class in the test set.  \n",
    "  Example:  \n",
    "  - Support for class **0** = how many passengers actually died in the test data.  \n",
    "  - Support for class **1** = how many passengers actually survived.\n",
    "\n",
    "### What the Rows Mean\n",
    "\n",
    "- **Row 0** → performance metrics for predicting class 0 (did not survive)  \n",
    "- **Row 1** → performance metrics for predicting class 1 (survived)\n",
    "\n",
    "### What the Columns Mean\n",
    "\n",
    "- **precision** → how correct positive predictions were for each class  \n",
    "- **recall** → how well the model found each class  \n",
    "- **f1-score** → combined score of precision + recall  \n",
    "- **support** → how many true examples there were for that class\n",
    "\n",
    "### Is `classification_report` a function?\n",
    "\n",
    "Yes.  \n",
    "It is a function in **scikit-learn** (`from sklearn.metrics import classification_report`) that calculates and prints all these metrics for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58f43-ea81-4049-9407-9502fcc56594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. EVALUATE THE MODEL\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy = (number of correct predictions) / (total predictions)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# classification_report shows precision, recall, and F1-score for each class\n",
    "print(\"Detailed classification report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a07b45-025f-4199-a2fc-58f80a4670df",
   "metadata": {},
   "source": [
    "## Step 11: Feature Importance\n",
    "\n",
    "Decision trees can tell us **which features mattered most** for the predictions.\n",
    "\n",
    "Feature importance answers:\n",
    "\n",
    "> “Which inputs did the model rely on the most when deciding who survived?”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8059c-a644-4006-91db-42e6c8f92324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. FEATURE IMPORTANCE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance scores from the trained model\n",
    "importances = model.feature_importances_\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for name, score in zip(feature_names, importances):\n",
    "    print(f\"{name}: {score:.3f}\")\n",
    "\n",
    "# Visualize feature importance as a horizontal bar chart\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance for Titanic Survival Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3ced5-4f5d-428a-9169-60e2f5c0bed9",
   "metadata": {},
   "source": [
    "## Step 12: Visualize the Decision Tree\n",
    "\n",
    "We can draw the decision tree to see the actual questions it asks, like:\n",
    "\n",
    "- \"Is `sex_male` <= 0.5?\"  \n",
    "- \"Is `pclass` <= 1.5?\"  \n",
    "- etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2951a4b-1ae1-4fef-afe3-bb3d34c600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. VISUALIZE (PLOT) THE DECISION TREE\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "tree_rules = export_text(model, feature_names=feature_names)\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277837dd-cac8-4b2e-8250-2b1dc3b5b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 v2. VISUALIZE (PLOT) THE DECISION TREE USING A FAMILIAR FORMAT\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Set up the figure size for the tree plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Draw the tree structure\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=feature_names,   # names of input features\n",
    "    class_names=[\"died\", \"survived\"],  # class labels for 0 and 1\n",
    "    filled=True,                  # color nodes by class\n",
    "    rounded=True,                 # rounded corners for nicer look\n",
    "    fontsize=9\n",
    ")\n",
    "\n",
    "plt.title(\"Decision Tree for Predicting Titanic Survival\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcabbe-4f52-46c9-a895-d62bbe455898",
   "metadata": {},
   "source": [
    "## Step 13: Predict a Single New Passenger (What-if Scenario)\n",
    "\n",
    "Finally, we can make a prediction for a **made-up passenger**.\n",
    "\n",
    "We have to pass the features in the **same order** as `feature_names`:\n",
    "\n",
    "`[pclass, age, sibsp, parch, fare, sex_male, embarked_Q, embarked_S]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7167e3-d0a7-4dd5-9c5f-6cc0a03790ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. USE THE MODEL TO PREDICT A NEW, MADE-UP PASSENGER\n",
    "\n",
    "# Example passenger:\n",
    "# - 3rd class\n",
    "# - 25 years old\n",
    "# - no siblings/spouses aboard\n",
    "# - no parents/children aboard\n",
    "# - fare = 7.25\n",
    "# - sex_male = 1 (male)\n",
    "# - embarked_Q = 0, embarked_S = 1  (so this means embarked at 'S')\n",
    "\n",
    "new_passenger = [[3, 25, 0, 0, 7.25, 1, 0, 1]]\n",
    "\n",
    "# Use the model to predict if this passenger would survive\n",
    "new_prediction = model.predict(new_passenger)\n",
    "\n",
    "print(\"Prediction for new passenger [3rd class, 25, male, embarked at S]:\")\n",
    "print(\"Survived?\" , \"Yes (1)\" if new_prediction[0] == 1 else \"No (0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228aa21-7dc7-460b-b304-cd10337f54ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
